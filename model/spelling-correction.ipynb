{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "파일명: finetune_llama_correction.py\n",
    "\n",
    "필요 라이브러리:\n",
    "pip install transformers accelerate bitsandbytes peft pandas scikit-learn\n",
    "\n",
    "- transformers : 모델/토크나이저 로드 및 Trainer\n",
    "- accelerate   : 다중 GPU/분산 학습 지원\n",
    "- bitsandbytes : 4bit/8bit 양자화\n",
    "- peft         : LoRA 같은 Parameter-Efficient Fine-Tuning\n",
    "- pandas, scikit-learn : 데이터 로드 & 전처리\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# bitsandbytes, huggingface\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "###############################################################################\n",
    "# 1) Config 클래스\n",
    "###############################################################################\n",
    "class Config:\n",
    "    # 모델 및 학습 관련\n",
    "    model_name = \"Bllossom/llama-3.2-Korean-Bllossom-3B\"\n",
    "    lora_r = 8\n",
    "    lora_alpha = 16\n",
    "    lora_dropout = 0.05\n",
    "    lora_bias = \"none\"\n",
    "    \n",
    "    # 데이터 관련\n",
    "    train_path = \"../database/train/train.csv\"\n",
    "    test_path  = \"../database/test/test.csv\"\n",
    "    \n",
    "    # 토크나이징, 최대 입력 길이\n",
    "    max_length = 512\n",
    "    \n",
    "    # 학습 파라미터\n",
    "    learning_rate = 1e-4\n",
    "    num_train_epochs = 1\n",
    "    per_device_train_batch_size = 1\n",
    "    per_device_eval_batch_size = 1\n",
    "    warmup_steps = 50\n",
    "    weight_decay = 0.01\n",
    "    \n",
    "    # 기타\n",
    "    output_dir = \"./finetuned-llama-correction\"\n",
    "    seed = 42\n",
    "    \n",
    "###############################################################################\n",
    "# 2) 노이즈 추가 함수 (데이터 증강)\n",
    "###############################################################################\n",
    "def add_noise_to_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    실제로는 자모/받침/오타 삽입 등 한글에 특화된 노이즈를 넣으면 좋습니다.\n",
    "    여기서는 간단히 한두 글자씩 랜덤 치환, 삽입, 삭제 정도만 예시로 넣습니다.\n",
    "    \"\"\"\n",
    "    # 노이즈를 추가할 수 있는 연산들\n",
    "    def random_char_swap(s):\n",
    "        if len(s) < 2:\n",
    "            return s\n",
    "        idx = random.randint(0, len(s) - 2)\n",
    "        # idx 글자와 idx+1 글자 위치를 바꾼다\n",
    "        lst = list(s)\n",
    "        lst[idx], lst[idx+1] = lst[idx+1], lst[idx]\n",
    "        return \"\".join(lst)\n",
    "    \n",
    "    def random_char_delete(s):\n",
    "        if not s:\n",
    "            return s\n",
    "        idx = random.randint(0, len(s)-1)\n",
    "        return s[:idx] + s[idx+1:]\n",
    "    \n",
    "    def random_char_replace(s):\n",
    "        if not s:\n",
    "            return s\n",
    "        idx = random.randint(0, len(s)-1)\n",
    "        # 임의의 유니코드 한글 범위 내에서 문자 생성(예: 44032(가) ~ 55203(힣))\n",
    "        random_kor_char = chr(random.randint(0xAC00, 0xD7A3))\n",
    "        s = list(s)\n",
    "        s[idx] = random_kor_char\n",
    "        return \"\".join(s)\n",
    "    \n",
    "    # 여러 개 노이즈 연산 중 무작위 선택\n",
    "    ops = [random_char_swap, random_char_delete, random_char_replace]\n",
    "    # 노이즈를 1~3회 적용 (상황에 따라 조절 가능)\n",
    "    num_noise_ops = random.randint(1, 3)\n",
    "    \n",
    "    noisy_text = text\n",
    "    for _ in range(num_noise_ops):\n",
    "        op = random.choice(ops)\n",
    "        noisy_text = op(noisy_text)\n",
    "        \n",
    "    return noisy_text\n",
    "\n",
    "###############################################################################\n",
    "# 3) Dataset 정의\n",
    "###############################################################################\n",
    "class CorrectionDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length: int=512, is_train: bool=True):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # 학습 시:   input  = 노이즈가 추가된 텍스트\n",
    "        #           label  = 원본 output\n",
    "        if self.is_train:\n",
    "            # train.csv: (row['input'], row['output'])  \n",
    "            # => 실제론 row['input']은 무시하고, row['output']에 노이즈를 입히고자 함\n",
    "            original_text = str(row['output'])\n",
    "            noisy_text = add_noise_to_text(original_text)\n",
    "            \n",
    "            # 프롬프트 형식(예시): \"아래 문장을 올바른 한국어로 교정하세요:\\n{noisy_text}\\n답변:\"\n",
    "            prompt_text = f\"아래 문장을 올바른 한국어로 교정하세요:\\n{noisy_text}\\n답변:\"\n",
    "            target_text = original_text  # 모델이 최종적으로 생성해야 하는 문장\n",
    "\n",
    "            # 모델 입력을 만들기 위해 prompt + target 형식으로 붙임\n",
    "            # causal LM에서는 label을 그대로 shift시켜 예측하기 때문에,\n",
    "            # prompt+target 전체를 입력으로 두고, target 부분만 loss 계산하도록 구성\n",
    "            # 보통 special token이나 구분자를 추가하는 방식을 사용하기도 함\n",
    "            full_text = prompt_text + \" \" + target_text\n",
    "            \n",
    "            tokenized = self.tokenizer(\n",
    "                full_text,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "            input_ids = tokenized[\"input_ids\"].squeeze(0)\n",
    "            attention_mask = tokenized[\"attention_mask\"].squeeze(0)\n",
    "            \n",
    "            # 어디부터 target(라벨)로 삼을지 구분해야 함\n",
    "            # 예시로 prompt 부분은 -100으로 ignore하고, target 부분만 정답으로 사용\n",
    "            prompt_len = len(self.tokenizer(prompt_text, add_special_tokens=False)[\"input_ids\"])\n",
    "            \n",
    "            labels = input_ids.clone()\n",
    "            # prompt 위치까지는 -100으로 마스킹하여 loss 미계산\n",
    "            labels[:prompt_len] = -100\n",
    "\n",
    "            return {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"labels\": labels\n",
    "            }\n",
    "        else:\n",
    "            # 실제 추론 시: row['input'] 만 가지고 inference\n",
    "            # 다만 Trainer eval 단계 등에서 label이 필요할 수 있으므로\n",
    "            # 여기서는 임시로 (no label) 처리\n",
    "            input_text = str(row['input'])\n",
    "            prompt_text = f\"아래 문장을 올바른 한국어로 교정하세요:\\n{input_text}\\n답변:\"\n",
    "\n",
    "            tokenized = self.tokenizer(\n",
    "                prompt_text,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "            input_ids = tokenized[\"input_ids\"].squeeze(0)\n",
    "            attention_mask = tokenized[\"attention_mask\"].squeeze(0)\n",
    "            \n",
    "            return {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask\n",
    "                # labels가 필요하다면, row에 'output' 컬럼이 있을 경우 추가\n",
    "            }\n",
    "\n",
    "###############################################################################\n",
    "# 4) 데이터 준비 함수\n",
    "###############################################################################\n",
    "def load_datasets(config: Config, tokenizer):\n",
    "    # train.csv 로드 => ID, input, output\n",
    "    df_train = pd.read_csv(config.train_path)\n",
    "    # test.csv  => ID, input (output 없음)\n",
    "    df_test  = pd.read_csv(config.test_path)\n",
    "    \n",
    "    # 예시로 train/valid split\n",
    "    df_train, df_valid = train_test_split(df_train, test_size=0.1, random_state=config.seed)\n",
    "    \n",
    "    train_dataset = CorrectionDataset(df_train, tokenizer, config.max_length, is_train=True)\n",
    "    valid_dataset = CorrectionDataset(df_valid, tokenizer, config.max_length, is_train=True)  # 검증 시에도 label 필요\n",
    "    test_dataset  = CorrectionDataset(df_test, tokenizer, config.max_length, is_train=False)\n",
    "    \n",
    "    return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "###############################################################################\n",
    "# 5) 모델 준비 (4bit 로 로드 + LoRA)\n",
    "###############################################################################\n",
    "def get_model(config: Config):\n",
    "    # (1) 토크나이저 로드\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config.model_name,\n",
    "        trust_remote_code=True  # Bllossom LLaMA 커스텀 코드 필요 시\n",
    "    )\n",
    "    tokenizer.pad_token_id = 0\n",
    "\n",
    "    # (2) 4bit 양자화 모델 로드\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.model_name,\n",
    "        trust_remote_code=True,\n",
    "        load_in_4bit=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # (3) kbit(4bit) 훈련 준비\n",
    "    # 이 단계가 누락되면 미세 튜닝 시 grad_fn이 사라져\n",
    "    # \"does not require grad\" 오류가 발생할 수 있음\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # (4) LoRA 설정\n",
    "    lora_config = LoraConfig(\n",
    "        r=config.lora_r,\n",
    "        lora_alpha=config.lora_alpha,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],  # LLaMA의 Q, V 부분에 적용\n",
    "        lora_dropout=config.lora_dropout,\n",
    "        bias=config.lora_bias,\n",
    "        task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "\n",
    "    # (5) LoRA 적용\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "###############################################################################\n",
    "# 6) 학습 함수\n",
    "###############################################################################\n",
    "def train_model(config: Config):\n",
    "    tokenizer, model = get_model(config)\n",
    "    train_dataset, valid_dataset, _ = load_datasets(config, tokenizer)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=config.output_dir,\n",
    "        num_train_epochs=config.num_train_epochs,\n",
    "        learning_rate=config.learning_rate,\n",
    "        per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "        warmup_steps=config.warmup_steps,\n",
    "        weight_decay=config.weight_decay,\n",
    "        evaluation_strategy=\"steps\",  # 혹은 \"epoch\"\n",
    "        save_steps=50,                # 모델 저장 빈도\n",
    "        eval_steps=50,                # 평가 빈도\n",
    "        logging_steps=10,\n",
    "        save_total_limit=2,\n",
    "        fp16=True,                    # FP16 사용\n",
    "        # or bf16=True (가능한 환경이면) \n",
    "        gradient_checkpointing=True,  # 메모리 절약\n",
    "        remove_unused_columns=False   # Dataset에서 입력 컬럼 그대로 사용\n",
    "    )\n",
    "\n",
    "    def data_collator(features):\n",
    "        # huggingface Trainer용 default DataCollator:\n",
    "        # 이미 모든 텐서가 padding되어 있다고 가정\n",
    "        return {\n",
    "            \"input_ids\": torch.stack([f[\"input_ids\"] for f in features]),\n",
    "            \"attention_mask\": torch.stack([f[\"attention_mask\"] for f in features]),\n",
    "            \"labels\": torch.stack([f[\"labels\"] for f in features])\n",
    "        }\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # 학습 완료 후 모델 저장\n",
    "    trainer.save_model(config.output_dir)\n",
    "    tokenizer.save_pretrained(config.output_dir)\n",
    "\n",
    "###############################################################################\n",
    "# 7) 추론(후처리) 함수\n",
    "###############################################################################\n",
    "def correct_sentence(tokenizer, model, text: str, max_new_tokens=128):\n",
    "    \"\"\"\n",
    "    주어진 text를 모델에 넣어서 교정 결과를 생성하는 함수.\n",
    "    text는 noisy한 문장(이미 LSTM 등에서 나온 예측값)이라고 가정.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    prompt = f\"아래 문장을 올바른 한국어로 교정하세요:\\n{text}\\n답변:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generation = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8\n",
    "        )\n",
    "    # 모델이 만든 전체 문장을 디코딩\n",
    "    output = tokenizer.decode(generation[0], skip_special_tokens=True)\n",
    "    \n",
    "    # prompt를 제외한 부분만 추출(간단히 문자열 치환)\n",
    "    # \"아래 문장을 ...\\n답변:\" 까지 잘라내기\n",
    "    if \"답변:\" in output:\n",
    "        output = output.split(\"답변:\")[-1].strip()\n",
    "    return output\n",
    "\n",
    "def inference_on_test(config: Config):\n",
    "    # 1) 모델 로드\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(config.output_dir)\n",
    "    tokenizer.pad_token_id = 0\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        config.output_dir,\n",
    "        load_in_4bit=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # LoRA 가중치도 불러오기 위해 peft 방법 사용\n",
    "    # 만약 Trainer.save_model()으로 LoRA가 같이 저장되었다면 자동 로드 가능\n",
    "    # (필요 시 peft 로드 별도)\n",
    "    \n",
    "    model.eval()  # 추론 모드\n",
    "    \n",
    "    # 2) test 데이터 로드\n",
    "    df_test = pd.read_csv(config.test_path)\n",
    "    \n",
    "    # 3) 기존 LSTM 모델의 결과물을 얻는 함수 (데모용)\n",
    "    # 실제론 사용자가 만든 BiMultiLSTMModel로 예측\n",
    "    def bimulti_lstm_predict(noisy_input: str) -> str:\n",
    "        # 여기서는 데모용으로 \"noisy_input\" 그대로 반환하거나\n",
    "        # 임의로 글자를 조금 바꿔 반환하는 방식\n",
    "        # 실제로는 사용자 LSTM 모델의 output이 들어와야 함\n",
    "        return noisy_input\n",
    "    \n",
    "    # 4) 각 문장에 대해 (1) LSTM 예측 -> (2) LLaMA 교정 -> (정답) 은 여기선 X\n",
    "    for i, row in df_test.iterrows():\n",
    "        input_text = str(row[\"input\"])\n",
    "        \n",
    "        # (1) LSTM 예측\n",
    "        lstm_output = bimulti_lstm_predict(input_text)\n",
    "        \n",
    "        # (2) LLaMA 교정\n",
    "        pred_str = correct_sentence(tokenizer, model, lstm_output)\n",
    "        \n",
    "        # (3) 정답(lab_str)은 없는 상태이므로 None 처리\n",
    "        lab_str = None  # 실제로 정답이 있으면 대입\n",
    "        \n",
    "        print(f\"[예측] {pred_str} (정답: {lab_str})\")\n",
    "\n",
    "###############################################################################\n",
    "# main 실행부\n",
    "###############################################################################\n",
    "\n",
    "config = Config()\n",
    "random.seed(config.seed)\n",
    "torch.manual_seed(config.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(config.seed)\n",
    "\n",
    "# 1) 모델 학습\n",
    "print(\"===== 1) 모델 학습 시작 =====\")\n",
    "train_model(config)\n",
    "\n",
    "# # 2) 학습 완료 후, test set에 대한 후처리 추론\n",
    "# print(\"===== 2) 테스트 데이터 추론(후처리) =====\")\n",
    "# inference_on_test(config)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
