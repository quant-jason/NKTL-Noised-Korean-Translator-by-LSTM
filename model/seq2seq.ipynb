{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 2475\n",
      "=== Train CBOW on GPU ===\n",
      "[CBOW Epoch 1/100] Loss: 4.6430\n",
      "[CBOW Epoch 2/100] Loss: 4.0833\n",
      "[CBOW Epoch 3/100] Loss: 3.9199\n",
      "[CBOW Epoch 4/100] Loss: 3.8184\n",
      "[CBOW Epoch 5/100] Loss: 3.7451\n",
      "[CBOW Epoch 6/100] Loss: 3.6884\n",
      "[CBOW Epoch 7/100] Loss: 3.6417\n",
      "[CBOW Epoch 8/100] Loss: 3.6026\n",
      "[CBOW Epoch 9/100] Loss: 3.5691\n",
      "[CBOW Epoch 10/100] Loss: 3.5395\n",
      "[CBOW Epoch 11/100] Loss: 3.5128\n",
      "[CBOW Epoch 12/100] Loss: 3.4890\n",
      "[CBOW Epoch 13/100] Loss: 3.4672\n",
      "[CBOW Epoch 14/100] Loss: 3.4472\n",
      "[CBOW Epoch 15/100] Loss: 3.4290\n",
      "[CBOW Epoch 16/100] Loss: 3.4118\n",
      "[CBOW Epoch 17/100] Loss: 3.3959\n",
      "[CBOW Epoch 18/100] Loss: 3.3810\n",
      "[CBOW Epoch 19/100] Loss: 3.3670\n",
      "[CBOW Epoch 20/100] Loss: 3.3540\n",
      "[CBOW Epoch 21/100] Loss: 3.3418\n",
      "[CBOW Epoch 22/100] Loss: 3.3296\n",
      "[CBOW Epoch 23/100] Loss: 3.3187\n",
      "[CBOW Epoch 24/100] Loss: 3.3082\n",
      "[CBOW Epoch 25/100] Loss: 3.2982\n",
      "[CBOW Epoch 26/100] Loss: 3.2886\n",
      "[CBOW Epoch 27/100] Loss: 3.2795\n",
      "[CBOW Epoch 28/100] Loss: 3.2708\n",
      "[CBOW Epoch 29/100] Loss: 3.2624\n",
      "[CBOW Epoch 30/100] Loss: 3.2545\n",
      "[CBOW Epoch 31/100] Loss: 3.2468\n",
      "[CBOW Epoch 32/100] Loss: 3.2394\n",
      "[CBOW Epoch 33/100] Loss: 3.2325\n",
      "[CBOW Epoch 34/100] Loss: 3.2258\n",
      "[CBOW Epoch 35/100] Loss: 3.2193\n",
      "[CBOW Epoch 36/100] Loss: 3.2132\n",
      "[CBOW Epoch 37/100] Loss: 3.2071\n",
      "[CBOW Epoch 38/100] Loss: 3.2018\n",
      "[CBOW Epoch 39/100] Loss: 3.1958\n",
      "[CBOW Epoch 40/100] Loss: 3.1906\n",
      "[CBOW Epoch 41/100] Loss: 3.1852\n",
      "[CBOW Epoch 42/100] Loss: 3.1804\n",
      "[CBOW Epoch 43/100] Loss: 3.1755\n",
      "[CBOW Epoch 44/100] Loss: 3.1710\n",
      "[CBOW Epoch 45/100] Loss: 3.1665\n",
      "[CBOW Epoch 46/100] Loss: 3.1621\n",
      "[CBOW Epoch 47/100] Loss: 3.1581\n",
      "[CBOW Epoch 48/100] Loss: 3.1538\n",
      "[CBOW Epoch 49/100] Loss: 3.1502\n",
      "[CBOW Epoch 50/100] Loss: 3.1461\n",
      "[CBOW Epoch 51/100] Loss: 3.1424\n",
      "[CBOW Epoch 52/100] Loss: 3.1390\n",
      "[CBOW Epoch 53/100] Loss: 3.1355\n",
      "[CBOW Epoch 54/100] Loss: 3.1323\n",
      "[CBOW Epoch 55/100] Loss: 3.1291\n",
      "[CBOW Epoch 56/100] Loss: 3.1258\n",
      "[CBOW Epoch 57/100] Loss: 3.1229\n",
      "[CBOW Epoch 58/100] Loss: 3.1198\n",
      "[CBOW Epoch 59/100] Loss: 3.1170\n",
      "[CBOW Epoch 60/100] Loss: 3.1141\n",
      "[CBOW Epoch 61/100] Loss: 3.1113\n",
      "[CBOW Epoch 62/100] Loss: 3.1084\n",
      "[CBOW Epoch 63/100] Loss: 3.1061\n",
      "[CBOW Epoch 64/100] Loss: 3.1034\n",
      "[CBOW Epoch 65/100] Loss: 3.1011\n",
      "[CBOW Epoch 66/100] Loss: 3.0985\n",
      "[CBOW Epoch 67/100] Loss: 3.0966\n",
      "[CBOW Epoch 68/100] Loss: 3.0943\n",
      "[CBOW Epoch 69/100] Loss: 3.0919\n",
      "[CBOW Epoch 70/100] Loss: 3.0898\n",
      "[CBOW Epoch 71/100] Loss: 3.0877\n",
      "[CBOW Epoch 72/100] Loss: 3.0860\n",
      "[CBOW Epoch 73/100] Loss: 3.0834\n",
      "[CBOW Epoch 74/100] Loss: 3.0819\n",
      "[CBOW Epoch 75/100] Loss: 3.0800\n",
      "[CBOW Epoch 76/100] Loss: 3.0779\n",
      "[CBOW Epoch 77/100] Loss: 3.0765\n",
      "[CBOW Epoch 78/100] Loss: 3.0746\n",
      "[CBOW Epoch 79/100] Loss: 3.0728\n",
      "[CBOW Epoch 80/100] Loss: 3.0713\n",
      "[CBOW Epoch 81/100] Loss: 3.0696\n",
      "[CBOW Epoch 82/100] Loss: 3.0680\n",
      "[CBOW Epoch 83/100] Loss: 3.0664\n",
      "[CBOW Epoch 84/100] Loss: 3.0652\n",
      "[CBOW Epoch 85/100] Loss: 3.0633\n",
      "[CBOW Epoch 86/100] Loss: 3.0623\n",
      "[CBOW Epoch 87/100] Loss: 3.0605\n",
      "[CBOW Epoch 88/100] Loss: 3.0592\n",
      "[CBOW Epoch 89/100] Loss: 3.0578\n",
      "[CBOW Epoch 90/100] Loss: 3.0565\n",
      "[CBOW Epoch 91/100] Loss: 3.0553\n",
      "[CBOW Epoch 92/100] Loss: 3.0539\n",
      "[CBOW Epoch 93/100] Loss: 3.0529\n",
      "[CBOW Epoch 94/100] Loss: 3.0514\n",
      "[CBOW Epoch 95/100] Loss: 3.0502\n",
      "[CBOW Epoch 96/100] Loss: 3.0490\n",
      "[CBOW Epoch 97/100] Loss: 3.0479\n",
      "[CBOW Epoch 98/100] Loss: 3.0469\n",
      "[CBOW Epoch 99/100] Loss: 3.0458\n",
      "[CBOW Epoch 100/100] Loss: 3.0447\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#######################################\n",
    "# Config\n",
    "#######################################\n",
    "class Config:\n",
    "    seed = 42\n",
    "    \n",
    "    train_path = \"../database/train/train.csv\"\n",
    "    test_path = \"../database/test/test.csv\"\n",
    "    \n",
    "    # Seq2Seq 하이퍼파라미터\n",
    "    batch_size = 16\n",
    "    num_epochs = 50              # Seq2Seq 학습 에폭\n",
    "    embedding_dim = 1024\n",
    "    hidden_size = 1024\n",
    "    learning_rate = 1e-3\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    max_length = 512\n",
    "    \n",
    "    # Special tokens\n",
    "    PAD_TOKEN = \"[PAD]\"\n",
    "    UNK_TOKEN = \"[UNK]\"\n",
    "    CLS_TOKEN = \"[CLS]\"\n",
    "    SEP_TOKEN = \"[SEP]\"\n",
    "    \n",
    "    # CBOW(Word2Vec) 하이퍼파라미터\n",
    "    w2v_epochs = 100\n",
    "    w2v_window = 5\n",
    "    w2v_min_count = 1\n",
    "\n",
    "    # **Teacher Forcing Ratio** 추가 (여기서 조절)\n",
    "    teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "#######################################\n",
    "# 1) Vocab 생성 (train.csv 기준)\n",
    "#######################################\n",
    "def build_train_based_vocab(train_path):\n",
    "    df = pd.read_csv(train_path)\n",
    "    train_unique_chars = set()\n",
    "    for idx, row in df.iterrows():\n",
    "        input_text = str(row[\"input\"])\n",
    "        output_text = str(row[\"output\"])\n",
    "        train_unique_chars.update(list(input_text))\n",
    "        train_unique_chars.update(list(output_text))\n",
    "\n",
    "    # 특수 토큰\n",
    "    special_tokens = [Config.PAD_TOKEN, Config.UNK_TOKEN, Config.CLS_TOKEN, Config.SEP_TOKEN]\n",
    "\n",
    "    # # 한글 완성형(가~힣)\n",
    "    # hangul_syllables = [chr(code) for code in range(0xAC00, 0xD7A4)]\n",
    "    # # 한글 자모\n",
    "    # hangul_jamos = (\n",
    "    #     \"ㄱㄲㄴㄷㄸㄹㅁㅂㅃㅅㅆㅇㅈㅉㅊㅋㅌㅍㅎ\"\n",
    "    #     + \"ㅏㅑㅓㅕㅗㅛㅜㅠㅡㅣ\"\n",
    "    #     + \"ㅐㅔㅒㅖㅘㅙㅚㅝㅞㅟㅢ\"\n",
    "    # )\n",
    "    # # 영어, 숫자, 특수문자, 공백\n",
    "    # extra_chars = string.punctuation + string.digits + string.ascii_letters + \" \"\n",
    "\n",
    "    # base_set = set(hangul_syllables) | set(hangul_jamos) | set(extra_chars)\n",
    "    # final_chars = train_unique_chars.union(base_set)\n",
    "    final_chars = train_unique_chars\n",
    "    vocab = {}\n",
    "    for token in special_tokens:\n",
    "        vocab[token] = len(vocab)\n",
    "    for ch in final_chars:\n",
    "        if ch not in vocab:\n",
    "            vocab[ch] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "#######################################\n",
    "# 2) Tokenizer\n",
    "#######################################\n",
    "class SyllableTokenizer:\n",
    "    def __init__(self, vocab,\n",
    "                 pad_token=\"[PAD]\",\n",
    "                 unk_token=\"[UNK]\",\n",
    "                 cls_token=\"[CLS]\",\n",
    "                 sep_token=\"[SEP]\"):\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        self.cls_token = cls_token\n",
    "        self.sep_token = sep_token\n",
    "        \n",
    "        self.pad_token_id = self.vocab.get(self.pad_token, 0)\n",
    "        self.unk_token_id = self.vocab.get(self.unk_token, 1)\n",
    "        self.cls_token_id = self.vocab.get(self.cls_token, 2)\n",
    "        self.sep_token_id = self.vocab.get(self.sep_token, 3)\n",
    "        \n",
    "        self.ids_to_token = {i: t for t, i in self.vocab.items()}\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return list(text)\n",
    "\n",
    "    def encode(self, text, add_special_tokens=True, max_length=None):\n",
    "        tokens = self.tokenize(text)\n",
    "        if add_special_tokens:\n",
    "            tokens = [self.cls_token] + tokens + [self.sep_token]\n",
    "        \n",
    "        if max_length is not None and len(tokens) > max_length:\n",
    "            tokens = tokens[:max_length]\n",
    "        \n",
    "        return [self.vocab.get(token, self.unk_token_id) for token in tokens]\n",
    "\n",
    "    def decode(self, token_ids, skip_special_tokens=True):\n",
    "        tokens = [self.ids_to_token.get(id_, self.unk_token) for id_ in token_ids]\n",
    "        if skip_special_tokens:\n",
    "            tokens = [t for t in tokens if t not in {self.cls_token, self.sep_token, self.pad_token}]\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "    def __call__(self, text, add_special_tokens=True, max_length=None):\n",
    "        return self.encode(text, add_special_tokens=add_special_tokens, max_length=max_length)\n",
    "\n",
    "\n",
    "#######################################\n",
    "# 3) Custom CBOW Dataset (for Word2Vec)\n",
    "#######################################\n",
    "class CBOWDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, corpus, word2idx, window_size=2):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        self.word2idx = word2idx\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        for sentence in corpus:\n",
    "            if len(sentence) < 2*window_size + 1:\n",
    "                continue\n",
    "            for i in range(window_size, len(sentence) - window_size):\n",
    "                center = sentence[i]\n",
    "                context_left = sentence[i-window_size : i]\n",
    "                context_right = sentence[i+1 : i+1+window_size]\n",
    "                context = context_left + context_right\n",
    "                self.data.append((center, context))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        center, context = self.data[idx]\n",
    "        center_id = self.word2idx.get(center, self.word2idx[Config.UNK_TOKEN])\n",
    "        context_ids = [self.word2idx.get(w, self.word2idx[Config.UNK_TOKEN]) for w in context]\n",
    "        return torch.tensor(center_id, dtype=torch.long), torch.tensor(context_ids, dtype=torch.long)\n",
    "\n",
    "\n",
    "def cbow_collate_fn(batch):\n",
    "    center_list = []\n",
    "    context_list = []\n",
    "    for c, ctx in batch:\n",
    "        center_list.append(c)\n",
    "        context_list.append(ctx)\n",
    "    \n",
    "    center_tensor = torch.stack(center_list, dim=0)  # (B,)\n",
    "    context_tensor = torch.stack(context_list, dim=0) # (B, 2*window_size)\n",
    "    \n",
    "    return center_tensor, context_tensor\n",
    "\n",
    "\n",
    "#######################################\n",
    "# 4) CBOW Model in PyTorch\n",
    "#######################################\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.linear = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, context_ids):\n",
    "        embedded = self.embedding(context_ids)    # (B, 2W, E)\n",
    "        avg_embed = embedded.mean(dim=1)          # (B, E)\n",
    "        logits = self.linear(avg_embed)           # (B, vocab_size)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def train_cbow_model(corpus, word2idx, vocab_size, embed_dim, device,\n",
    "                     window=2, epochs=100, batch_size=1024):\n",
    "    dataset = CBOWDataset(corpus, word2idx, window_size=window)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size,\n",
    "                        shuffle=True, collate_fn=cbow_collate_fn)\n",
    "    \n",
    "    model = CBOWModel(vocab_size, embed_dim, pad_idx=word2idx[Config.PAD_TOKEN]).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=word2idx[Config.PAD_TOKEN])\n",
    "    \n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for center_ids, context_ids in loader:\n",
    "            center_ids = center_ids.to(device)       # (B,)\n",
    "            context_ids = context_ids.to(device)     # (B, 2W)\n",
    "            \n",
    "            logits = model(context_ids)              # (B, vocab_size)\n",
    "            loss = criterion(logits, center_ids)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(loader) if len(loader) > 0 else 0\n",
    "        print(f\"[CBOW Epoch {ep}/{epochs}] Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "#######################################\n",
    "# 5) Dataset for Seq2Seq\n",
    "#######################################\n",
    "class ObfuscatedKoreanDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=None, is_train=True):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        input_text = str(row[\"input\"]).rstrip()\n",
    "        input_ids = self.tokenizer.encode(\n",
    "            input_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        if self.is_train:\n",
    "            output_text = str(row[\"output\"]).rstrip()\n",
    "            output_ids = self.tokenizer.encode(\n",
    "                output_text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_length\n",
    "            )\n",
    "            return {\n",
    "                \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                \"output_ids\": torch.tensor(output_ids, dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                \"ID\": row[\"ID\"]\n",
    "            }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    has_output = \"output_ids\" in batch[0]\n",
    "    input_ids_list = []\n",
    "    output_ids_list = []\n",
    "    IDs_list = []\n",
    "    \n",
    "    for item in batch:\n",
    "        input_ids_list.append(item[\"input_ids\"])\n",
    "        if has_output:\n",
    "            output_ids_list.append(item[\"output_ids\"])\n",
    "        else:\n",
    "            IDs_list.append(item[\"ID\"])\n",
    "    \n",
    "    input_ids_padded = nn.utils.rnn.pad_sequence(input_ids_list, batch_first=True, padding_value=0)\n",
    "    \n",
    "    if has_output:\n",
    "        output_ids_padded = nn.utils.rnn.pad_sequence(output_ids_list, batch_first=True, padding_value=0)\n",
    "        return input_ids_padded, output_ids_padded\n",
    "    else:\n",
    "        return input_ids_padded, IDs_list\n",
    "\n",
    "\n",
    "#######################################\n",
    "# 6) Seq2Seq (GRU) 모델\n",
    "#######################################\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, hidden = self.gru(embedded, hidden)\n",
    "        return outputs, hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward_step(self, input_token, hidden):\n",
    "        \"\"\"\n",
    "        input_token: (B, 1)\n",
    "        hidden: (1, B, hidden_size)\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_token)   # (B,1,E)\n",
    "        outputs, hidden = self.gru(embedded, hidden)  # (B,1,H)\n",
    "        logits = self.out(outputs)  # (B,1,vocab_size)\n",
    "        return logits, hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, pad_idx):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(vocab_size, embed_dim, hidden_size, pad_idx)\n",
    "        self.decoder = Decoder(vocab_size, embed_dim, hidden_size, pad_idx)\n",
    "    \n",
    "    def encode(self, src_ids):\n",
    "        return self.encoder(src_ids)\n",
    "\n",
    "    # 기존 self.forward(...)은 사용하지 않고,\n",
    "    # Teacher Forcing용 step-by-step 방식은 학습 루프에서 직접 구현.\n",
    "    # (원한다면 아래 forward를 그대로 두어도 되지만, 여기선 생략)\n",
    "    # def forward(self, src_ids, tgt_ids):\n",
    "    #     pass\n",
    "\n",
    "\n",
    "#######################################\n",
    "# 7) Training Loop for Seq2Seq (Teacher Forcing Ratio 적용)\n",
    "#######################################\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion, device, teacher_forcing_ratio=0.2):\n",
    "    \"\"\"\n",
    "    - Encoder -> hidden\n",
    "    - Decoder를 time-step별로 호출하면서,\n",
    "      teacher forcing 비율(teacher_forcing_ratio)로 정답 토큰 vs 이전 예측 토큰을 섞어서 입력\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids, output_ids = batch  # (B, T_in), (B, T_out)\n",
    "        input_ids = input_ids.to(device)\n",
    "        output_ids = output_ids.to(device)\n",
    "\n",
    "        # 1) 인코더\n",
    "        _, hidden = model.encode(input_ids)  # hidden: (1, B, H)\n",
    "        B, T_out = output_ids.shape\n",
    "\n",
    "        # 2) 디코더를 time-step별로 호출\n",
    "        # 첫 token은 output_ids[:, 0] (ex: [CLS])라 가정\n",
    "        dec_input = output_ids[:, 0].unsqueeze(1)  # (B,1)\n",
    "        \n",
    "        # 로짓을 저장할 tensor\n",
    "        # (B,T_out, vocab_size)\n",
    "        all_logits = torch.zeros(B, T_out, model.decoder.out.out_features, device=device)\n",
    "\n",
    "        for t in range(1, T_out):\n",
    "            logits_t, hidden = model.decoder.forward_step(dec_input, hidden)\n",
    "            # logits_t: (B,1,vocab_size)\n",
    "            all_logits[:, t, :] = logits_t[:, 0, :]  # (B,vocab_size)에 해당\n",
    "\n",
    "            # 다음 step의 dec_input 결정 (teacher forcing)\n",
    "            use_tf = (random.random() < teacher_forcing_ratio)\n",
    "            if use_tf:\n",
    "                # 정답 토큰 사용\n",
    "                dec_input = output_ids[:, t].unsqueeze(1)  # (B,1)\n",
    "            else:\n",
    "                # 이전 예측을 입력\n",
    "                dec_input = logits_t.argmax(dim=-1)  # (B,1)\n",
    "        \n",
    "        # 3) Loss 계산 (t=0 부분은 대부분 [CLS]이므로 t=1~T_out-1만 계산하거나, 그냥 전체 계산)\n",
    "        vocab_size = all_logits.shape[-1]\n",
    "        # 아래에서는 전체 cross-entropy 계산\n",
    "        loss = criterion(\n",
    "            all_logits.view(-1, vocab_size), \n",
    "            output_ids.view(-1)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def train_model(model, train_loader, config):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # pad=0\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        avg_loss = train_one_epoch(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            device=config.device,\n",
    "            teacher_forcing_ratio=Config.teacher_forcing_ratio\n",
    "        )\n",
    "        print(f\"[Epoch {epoch+1}/{config.num_epochs}] Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "#######################################\n",
    "# 8) Inference\n",
    "#######################################\n",
    "def greedy_decode(model, src_ids, tokenizer, max_len, pad_idx, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, hidden = model.encode(src_ids.to(device))  # (1,B,H)\n",
    "        \n",
    "        B = src_ids.size(0)\n",
    "        generated_ids = []\n",
    "        \n",
    "        # 첫 입력을 [CLS]라고 가정\n",
    "        dec_input = torch.tensor([tokenizer.cls_token_id]*B, dtype=torch.long, device=device).unsqueeze(1)\n",
    "        \n",
    "        for t in range(max_len - 1):\n",
    "            logits_t, hidden = model.decoder.forward_step(dec_input, hidden)  # (B,1,vocab)\n",
    "            next_token_id = logits_t.argmax(-1)  # (B,1)\n",
    "            generated_ids.append(next_token_id[:,0])  # list of (B,) tensors\n",
    "\n",
    "            dec_input = next_token_id  # next_input\n",
    "        \n",
    "        # (max_len-1, B) 형태 -> (B, max_len-1)\n",
    "        # 스택 후, transpose\n",
    "        all_gen = torch.stack(generated_ids, dim=0).transpose(0,1)  # (B, max_len-1)\n",
    "\n",
    "        # 여기서는 batch_size=1씩 돌린다고 했으므로, all_gen[0] 사용\n",
    "        # 만약 배치 처리를 하려면 for문으로 각각 디코딩\n",
    "        seq_out = all_gen[0].tolist()  # 첫 배치\n",
    "        text = tokenizer.decode(seq_out, skip_special_tokens=True)\n",
    "        return text\n",
    "\n",
    "def inference(model, test_loader, tokenizer, config):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for batch in test_loader:\n",
    "        input_ids, IDs = batch\n",
    "        input_ids = input_ids.to(config.device)\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            single_src = input_ids[i].unsqueeze(0)\n",
    "            ID = IDs[i]\n",
    "            pred_text = greedy_decode(model, single_src, tokenizer, seq_len, tokenizer.pad_token_id, config.device)\n",
    "            results.append({\"ID\": ID, \"output\": pred_text})\n",
    "    return results\n",
    "\n",
    "\n",
    "#######################################\n",
    "# 9) Main\n",
    "#######################################\n",
    "\n",
    "\n",
    "# 시드 고정\n",
    "random.seed(Config.seed)\n",
    "np.random.seed(Config.seed)\n",
    "torch.manual_seed(Config.seed)\n",
    "torch.cuda.manual_seed_all(Config.seed)\n",
    "\n",
    "# 1) Vocab & Tokenizer\n",
    "vocab = build_train_based_vocab(Config.train_path)\n",
    "tokenizer = SyllableTokenizer(\n",
    "    vocab,\n",
    "    pad_token=Config.PAD_TOKEN,\n",
    "    unk_token=Config.UNK_TOKEN,\n",
    "    cls_token=Config.CLS_TOKEN,\n",
    "    sep_token=Config.SEP_TOKEN\n",
    ")\n",
    "print(f\"Vocab size = {len(vocab)}\")\n",
    "\n",
    "# 2) CBOW용 Corpus 만들기 (train.csv, input+output 모두)\n",
    "df_train = pd.read_csv(Config.train_path)\n",
    "corpus = []\n",
    "for idx, row in df_train.iterrows():\n",
    "    inp = str(row[\"input\"])\n",
    "    outp = str(row[\"output\"])\n",
    "    corpus.append(list(inp))\n",
    "    corpus.append(list(outp))\n",
    "\n",
    "# 3) CBOW 학습 (GPU에서)\n",
    "print(\"=== Train CBOW on GPU ===\")\n",
    "cbow_model = train_cbow_model(\n",
    "    corpus,\n",
    "    word2idx=vocab,\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=Config.embedding_dim,\n",
    "    device=Config.device,\n",
    "    window=Config.w2v_window,\n",
    "    epochs=Config.w2v_epochs,\n",
    "    batch_size=2048\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW embedding loaded into seq2seq model.\n",
      "[Epoch 1/50] Loss: 3.7861\n",
      "[Epoch 2/50] Loss: 3.4215\n",
      "[Epoch 3/50] Loss: 3.3040\n",
      "[Epoch 4/50] Loss: 3.2117\n",
      "[Epoch 5/50] Loss: 3.1427\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m ObfuscatedKoreanDataset(df_train, tokenizer, max_length\u001b[38;5;241m=\u001b[39mConfig\u001b[38;5;241m.\u001b[39mmax_length, is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     24\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mConfig\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n\u001b[1;32m---> 25\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mConfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 378\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, config)\u001b[0m\n\u001b[0;32m    375\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# pad=0\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_epochs):\n\u001b[1;32m--> 378\u001b[0m     avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mteacher_forcing_ratio\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 367\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, train_loader, optimizer, criterion, device, teacher_forcing_ratio)\u001b[0m\n\u001b[0;32m    361\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(\n\u001b[0;32m    362\u001b[0m     all_logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), \n\u001b[0;32m    363\u001b[0m     output_ids\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    364\u001b[0m )\n\u001b[0;32m    366\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 367\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    368\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    369\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:491\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    483\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    484\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    489\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    490\u001b[0m     )\n\u001b[1;32m--> 491\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 4) CBOW 모델 -> 임베딩 매트릭스 추출\n",
    "cbow_weights = cbow_model.embedding.weight.data.cpu().numpy()  # (vocab_size, embed_dim)\n",
    "\n",
    "# 5) Seq2Seq 모델 준비\n",
    "pad_idx = tokenizer.pad_token_id\n",
    "model = Seq2Seq(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=Config.embedding_dim,\n",
    "    hidden_size=Config.hidden_size,\n",
    "    pad_idx=pad_idx\n",
    ").to(Config.device)\n",
    "\n",
    "# 6) Seq2Seq 임베딩 레이어 초기화 (CBOW 결과)\n",
    "encoder_weight = torch.from_numpy(cbow_weights).float().to(Config.device)\n",
    "decoder_weight = torch.from_numpy(cbow_weights).float().to(Config.device)\n",
    "\n",
    "model.encoder.embedding.weight.data = encoder_weight\n",
    "model.decoder.embedding.weight.data = decoder_weight\n",
    "\n",
    "print(\"CBOW embedding loaded into seq2seq model.\")\n",
    "\n",
    "# 7) Seq2Seq 학습 (Teacher Forcing Ratio 적용)\n",
    "train_dataset = ObfuscatedKoreanDataset(df_train, tokenizer, max_length=Config.max_length, is_train=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "train_model(model, train_loader, Config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Inference\n",
    "df_test = pd.read_csv(Config.test_path)\n",
    "test_dataset = ObfuscatedKoreanDataset(df_test, tokenizer, max_length=Config.max_length, is_train=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "results = inference(model, test_loader, tokenizer, Config)\n",
    "\n",
    "# 9) 결과 일부 확인\n",
    "for r in results[:10]:\n",
    "    print(r[\"ID\"], \"=>\", r[\"output\"])\n",
    "\n",
    "# 10) 모델 저장\n",
    "torch.save(cbow_model.state_dict(), \"cbow_word2vec.pt\")\n",
    "torch.save(model.state_dict(), \"seq2seq_model.pt\")\n",
    "print(\"Models are saved: cbow_word2vec.pt, seq2seq_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pre-trained CBOW & Seq2Seq model.\n",
      "[Fine-tune Epoch 1/50] Loss: 4.5097\n",
      "[Fine-tune Epoch 2/50] Loss: 4.4681\n",
      "[Fine-tune Epoch 3/50] Loss: 4.4477\n",
      "[Fine-tune Epoch 4/50] Loss: 4.4291\n",
      "[Fine-tune Epoch 5/50] Loss: 4.4148\n",
      "[Fine-tune Epoch 6/50] Loss: 4.3978\n",
      "[Fine-tune Epoch 7/50] Loss: 4.3826\n",
      "[Fine-tune Epoch 8/50] Loss: 4.3715\n",
      "[Fine-tune Epoch 9/50] Loss: 4.3579\n",
      "[Fine-tune Epoch 10/50] Loss: 4.3468\n",
      "[Fine-tune Epoch 11/50] Loss: 4.3354\n",
      "[Fine-tune Epoch 12/50] Loss: 4.3267\n",
      "[Fine-tune Epoch 13/50] Loss: 4.3124\n",
      "[Fine-tune Epoch 14/50] Loss: 4.3011\n",
      "[Fine-tune Epoch 15/50] Loss: 4.2920\n",
      "[Fine-tune Epoch 16/50] Loss: 4.2802\n",
      "[Fine-tune Epoch 17/50] Loss: 4.2700\n",
      "[Fine-tune Epoch 18/50] Loss: 4.2609\n",
      "[Fine-tune Epoch 19/50] Loss: 4.2555\n",
      "[Fine-tune Epoch 20/50] Loss: 4.2420\n",
      "[Fine-tune Epoch 21/50] Loss: 4.2323\n",
      "[Fine-tune Epoch 22/50] Loss: 4.2254\n",
      "[Fine-tune Epoch 23/50] Loss: 4.2160\n",
      "[Fine-tune Epoch 24/50] Loss: 4.2067\n",
      "[Fine-tune Epoch 25/50] Loss: 4.1988\n",
      "[Fine-tune Epoch 26/50] Loss: 4.1889\n",
      "[Fine-tune Epoch 27/50] Loss: 4.1812\n",
      "[Fine-tune Epoch 28/50] Loss: 4.1786\n",
      "[Fine-tune Epoch 29/50] Loss: 4.1656\n",
      "[Fine-tune Epoch 30/50] Loss: 4.1600\n",
      "[Fine-tune Epoch 31/50] Loss: 4.1527\n",
      "[Fine-tune Epoch 32/50] Loss: 4.1453\n",
      "[Fine-tune Epoch 33/50] Loss: 4.1391\n",
      "[Fine-tune Epoch 34/50] Loss: 4.1300\n",
      "[Fine-tune Epoch 35/50] Loss: 4.1248\n",
      "[Fine-tune Epoch 36/50] Loss: 4.1188\n",
      "[Fine-tune Epoch 37/50] Loss: 4.1110\n",
      "[Fine-tune Epoch 38/50] Loss: 4.1090\n",
      "[Fine-tune Epoch 39/50] Loss: 4.0987\n",
      "[Fine-tune Epoch 40/50] Loss: 4.0936\n",
      "[Fine-tune Epoch 41/50] Loss: 4.0874\n",
      "[Fine-tune Epoch 42/50] Loss: 4.0801\n",
      "[Fine-tune Epoch 43/50] Loss: 4.0770\n",
      "[Fine-tune Epoch 44/50] Loss: 4.0734\n",
      "[Fine-tune Epoch 45/50] Loss: 4.0698\n",
      "[Fine-tune Epoch 46/50] Loss: 4.0625\n",
      "[Fine-tune Epoch 47/50] Loss: 4.0558\n",
      "[Fine-tune Epoch 48/50] Loss: 4.0474\n",
      "[Fine-tune Epoch 49/50] Loss: 4.0429\n",
      "[Fine-tune Epoch 50/50] Loss: 4.0391\n",
      "Fine-tuned model saved: seq2seq_finetuned.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "#######################################\n",
    "# Config (Fine-tuning)\n",
    "#######################################\n",
    "class FineTuneConfig:\n",
    "    seed = 42\n",
    "    train_path = \"../database/train/train.csv\"\n",
    "    batch_size = 16\n",
    "    num_epochs = 50  # 추가 학습할 epoch 수\n",
    "    learning_rate = 1e-4  # 기존보다 낮은 LR로 Fine-tuning\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    teacher_forcing_ratio = 0.0  # ✅ Teacher Forcing Ratio 낮춤\n",
    "\n",
    "\n",
    "#######################################\n",
    "# 1) 기존 모델 불러오기\n",
    "#######################################\n",
    "# 기존 vocab 및 tokenizer 사용\n",
    "vocab = build_train_based_vocab(FineTuneConfig.train_path)\n",
    "tokenizer = SyllableTokenizer(\n",
    "    vocab,\n",
    "    pad_token=Config.PAD_TOKEN,\n",
    "    unk_token=Config.UNK_TOKEN,\n",
    "    cls_token=Config.CLS_TOKEN,\n",
    "    sep_token=Config.SEP_TOKEN\n",
    ")\n",
    "\n",
    "# CBOW 모델 불러오기 (임베딩 재사용)\n",
    "cbow_model = CBOWModel(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=Config.embedding_dim,\n",
    "    pad_idx=tokenizer.pad_token_id\n",
    ").to(FineTuneConfig.device)\n",
    "cbow_model.load_state_dict(torch.load(\"cbow_word2vec.pt\"))\n",
    "cbow_model.eval()  # 임베딩만 사용할 것이므로 eval 모드\n",
    "\n",
    "# Seq2Seq 모델 불러오기\n",
    "model = Seq2Seq(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=Config.embedding_dim,\n",
    "    hidden_size=Config.hidden_size,\n",
    "    pad_idx=tokenizer.pad_token_id\n",
    ").to(FineTuneConfig.device)\n",
    "model.load_state_dict(torch.load(\"seq2seq_model.pt\"))\n",
    "\n",
    "# ✅ 기존 CBOW 임베딩을 다시 적용\n",
    "with torch.no_grad():\n",
    "    cbow_weights = cbow_model.embedding.weight.data.cpu().numpy()\n",
    "    model.encoder.embedding.weight.data = torch.from_numpy(cbow_weights).float().to(FineTuneConfig.device)\n",
    "    model.decoder.embedding.weight.data = torch.from_numpy(cbow_weights).float().to(FineTuneConfig.device)\n",
    "\n",
    "print(\"Loaded pre-trained CBOW & Seq2Seq model.\")\n",
    "\n",
    "#######################################\n",
    "# 2) Dataset & DataLoader 준비\n",
    "#######################################\n",
    "df_train = pd.read_csv(FineTuneConfig.train_path)\n",
    "train_dataset = ObfuscatedKoreanDataset(df_train, tokenizer, max_length=Config.max_length, is_train=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=FineTuneConfig.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "#######################################\n",
    "# 3) Fine-tuning Training Loop\n",
    "#######################################\n",
    "def fine_tune_one_epoch(model, train_loader, optimizer, criterion, device, teacher_forcing_ratio):\n",
    "    \"\"\"\n",
    "    기존 학습된 모델을 불러와 Teacher Forcing Ratio를 낮춰서 추가 학습하는 함수\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids, output_ids = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        output_ids = output_ids.to(device)\n",
    "\n",
    "        _, hidden = model.encode(input_ids)  # (1, B, H)\n",
    "        B, T_out = output_ids.shape\n",
    "\n",
    "        dec_input = output_ids[:, 0].unsqueeze(1)  # (B, 1)\n",
    "        all_logits = torch.zeros(B, T_out, model.decoder.out.out_features, device=device)\n",
    "\n",
    "        for t in range(1, T_out):\n",
    "            logits_t, hidden = model.decoder.forward_step(dec_input, hidden)\n",
    "            all_logits[:, t, :] = logits_t[:, 0, :]\n",
    "\n",
    "            use_tf = (random.random() < teacher_forcing_ratio)\n",
    "            if use_tf:\n",
    "                dec_input = output_ids[:, t].unsqueeze(1)  # (B, 1)\n",
    "            else:\n",
    "                dec_input = logits_t.argmax(dim=-1)  # (B, 1)\n",
    "\n",
    "        vocab_size = all_logits.shape[-1]\n",
    "        loss = criterion(\n",
    "            all_logits.view(-1, vocab_size), \n",
    "            output_ids.view(-1)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def fine_tune_model(model, train_loader, config):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # pad=0\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        avg_loss = fine_tune_one_epoch(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            device=config.device,\n",
    "            teacher_forcing_ratio=config.teacher_forcing_ratio\n",
    "        )\n",
    "        print(f\"[Fine-tune Epoch {epoch+1}/{config.num_epochs}] Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "# ✅ Fine-tuning 수행\n",
    "fine_tune_model(model, train_loader, FineTuneConfig)\n",
    "\n",
    "# ✅ Fine-tuned 모델 저장\n",
    "torch.save(model.state_dict(), \"seq2seq_finetuned.pt\")\n",
    "print(\"Fine-tuned model saved: seq2seq_finetuned.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Config\n",
    "#######################################\n",
    "class Config:\n",
    "    seed = 42\n",
    "    \n",
    "    train_path = \"../database/train/train.csv\"\n",
    "    test_path = \"../database/test/test.csv\"\n",
    "    \n",
    "    # Seq2Seq 하이퍼파라미터\n",
    "    batch_size = 16\n",
    "    num_epochs = 5              # Seq2Seq 학습 에폭\n",
    "    embedding_dim = 256\n",
    "    hidden_size = 512\n",
    "    learning_rate = 1e-3\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    max_length = 512\n",
    "    \n",
    "    # Special tokens\n",
    "    PAD_TOKEN = \"[PAD]\"\n",
    "    UNK_TOKEN = \"[UNK]\"\n",
    "    CLS_TOKEN = \"[CLS]\"\n",
    "    SEP_TOKEN = \"[SEP]\"\n",
    "    \n",
    "    # CBOW(Word2Vec) 하이퍼파라미터\n",
    "    w2v_epochs = 10\n",
    "    w2v_window = 2\n",
    "    w2v_min_count = 1\n",
    "\n",
    "    # **Teacher Forcing Ratio** 추가 (여기서 조절)\n",
    "    teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "#######################################\n",
    "# 1) Vocab 생성 (train.csv 기준)\n",
    "#######################################\n",
    "def build_train_based_vocab(train_path):\n",
    "    df = pd.read_csv(train_path)\n",
    "    train_unique_chars = set()\n",
    "    for idx, row in df.iterrows():\n",
    "        input_text = str(row[\"input\"])\n",
    "        output_text = str(row[\"output\"])\n",
    "        train_unique_chars.update(list(input_text))\n",
    "        train_unique_chars.update(list(output_text))\n",
    "\n",
    "    # 특수 토큰\n",
    "    special_tokens = [Config.PAD_TOKEN, Config.UNK_TOKEN, Config.CLS_TOKEN, Config.SEP_TOKEN]\n",
    "\n",
    "    # 한글 완성형(가~힣)\n",
    "    hangul_syllables = [chr(code) for code in range(0xAC00, 0xD7A4)]\n",
    "    # 한글 자모\n",
    "    hangul_jamos = (\n",
    "        \"ㄱㄲㄴㄷㄸㄹㅁㅂㅃㅅㅆㅇㅈㅉㅊㅋㅌㅍㅎ\"\n",
    "        + \"ㅏㅑㅓㅕㅗㅛㅜㅠㅡㅣ\"\n",
    "        + \"ㅐㅔㅒㅖㅘㅙㅚㅝㅞㅟㅢ\"\n",
    "    )\n",
    "    # 영어, 숫자, 특수문자, 공백\n",
    "    extra_chars = string.punctuation + string.digits + string.ascii_letters + \" \"\n",
    "\n",
    "    base_set = set(hangul_syllables) | set(hangul_jamos) | set(extra_chars)\n",
    "    final_chars = train_unique_chars.union(base_set)\n",
    "\n",
    "    vocab = {}\n",
    "    for token in special_tokens:\n",
    "        vocab[token] = len(vocab)\n",
    "    for ch in final_chars:\n",
    "        if ch not in vocab:\n",
    "            vocab[ch] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "#######################################\n",
    "# 2) Tokenizer\n",
    "#######################################\n",
    "class SyllableTokenizer:\n",
    "    def __init__(self, vocab,\n",
    "                 pad_token=\"[PAD]\",\n",
    "                 unk_token=\"[UNK]\",\n",
    "                 cls_token=\"[CLS]\",\n",
    "                 sep_token=\"[SEP]\"):\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        self.cls_token = cls_token\n",
    "        self.sep_token = sep_token\n",
    "        \n",
    "        self.pad_token_id = self.vocab.get(self.pad_token, 0)\n",
    "        self.unk_token_id = self.vocab.get(self.unk_token, 1)\n",
    "        self.cls_token_id = self.vocab.get(self.cls_token, 2)\n",
    "        self.sep_token_id = self.vocab.get(self.sep_token, 3)\n",
    "        \n",
    "        self.ids_to_token = {i: t for t, i in self.vocab.items()}\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return list(text)\n",
    "\n",
    "    def encode(self, text, add_special_tokens=True, max_length=None):\n",
    "        tokens = self.tokenize(text)\n",
    "        if add_special_tokens:\n",
    "            tokens = [self.cls_token] + tokens + [self.sep_token]\n",
    "        \n",
    "        if max_length is not None and len(tokens) > max_length:\n",
    "            tokens = tokens[:max_length]\n",
    "        \n",
    "        return [self.vocab.get(token, self.unk_token_id) for token in tokens]\n",
    "\n",
    "    def decode(self, token_ids, skip_special_tokens=True):\n",
    "        tokens = [self.ids_to_token.get(id_, self.unk_token) for id_ in token_ids]\n",
    "        if skip_special_tokens:\n",
    "            tokens = [t for t in tokens if t not in {self.cls_token, self.sep_token, self.pad_token}]\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "    def __call__(self, text, add_special_tokens=True, max_length=None):\n",
    "        return self.encode(text, add_special_tokens=add_special_tokens, max_length=max_length)\n",
    "\n",
    "\n",
    "#######################################\n",
    "# 3) Custom CBOW Dataset (for Word2Vec)\n",
    "#######################################\n",
    "class CBOWDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, corpus, word2idx, window_size=2):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        self.word2idx = word2idx\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        for sentence in corpus:\n",
    "            if len(sentence) < 2*window_size + 1:\n",
    "                continue\n",
    "            for i in range(window_size, len(sentence) - window_size):\n",
    "                center = sentence[i]\n",
    "                context_left = sentence[i-window_size : i]\n",
    "                context_right = sentence[i+1 : i+1+window_size]\n",
    "                context = context_left + context_right\n",
    "                self.data.append((center, context))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        center, context = self.data[idx]\n",
    "        center_id = self.word2idx.get(center, self.word2idx[Config.UNK_TOKEN])\n",
    "        context_ids = [self.word2idx.get(w, self.word2idx[Config.UNK_TOKEN]) for w in context]\n",
    "        return torch.tensor(center_id, dtype=torch.long), torch.tensor(context_ids, dtype=torch.long)\n",
    "\n",
    "\n",
    "def cbow_collate_fn(batch):\n",
    "    center_list = []\n",
    "    context_list = []\n",
    "    for c, ctx in batch:\n",
    "        center_list.append(c)\n",
    "        context_list.append(ctx)\n",
    "    \n",
    "    center_tensor = torch.stack(center_list, dim=0)  # (B,)\n",
    "    context_tensor = torch.stack(context_list, dim=0) # (B, 2*window_size)\n",
    "    \n",
    "    return center_tensor, context_tensor\n",
    "\n",
    "\n",
    "#######################################\n",
    "# 4) CBOW Model in PyTorch\n",
    "#######################################\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.linear = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, context_ids):\n",
    "        embedded = self.embedding(context_ids)    # (B, 2W, E)\n",
    "        avg_embed = embedded.mean(dim=1)          # (B, E)\n",
    "        logits = self.linear(avg_embed)           # (B, vocab_size)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def train_cbow_model(corpus, word2idx, vocab_size, embed_dim, device,\n",
    "                     window=2, epochs=10, batch_size=128):\n",
    "    dataset = CBOWDataset(corpus, word2idx, window_size=window)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size,\n",
    "                        shuffle=True, collate_fn=cbow_collate_fn)\n",
    "    \n",
    "    model = CBOWModel(vocab_size, embed_dim, pad_idx=word2idx[Config.PAD_TOKEN]).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=word2idx[Config.PAD_TOKEN])\n",
    "    \n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for center_ids, context_ids in loader:\n",
    "            center_ids = center_ids.to(device)       # (B,)\n",
    "            context_ids = context_ids.to(device)     # (B, 2W)\n",
    "            \n",
    "            logits = model(context_ids)              # (B, vocab_size)\n",
    "            loss = criterion(logits, center_ids)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(loader) if len(loader) > 0 else 0\n",
    "        print(f\"[CBOW Epoch {ep}/{epochs}] Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "#######################################\n",
    "# 5) Dataset for Seq2Seq\n",
    "#######################################\n",
    "class ObfuscatedKoreanDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=None, is_train=True):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        input_text = str(row[\"input\"]).rstrip()\n",
    "        input_ids = self.tokenizer.encode(\n",
    "            input_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        if self.is_train:\n",
    "            output_text = str(row[\"output\"]).rstrip()\n",
    "            output_ids = self.tokenizer.encode(\n",
    "                output_text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_length\n",
    "            )\n",
    "            return {\n",
    "                \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                \"output_ids\": torch.tensor(output_ids, dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                \"ID\": row[\"ID\"]\n",
    "            }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    has_output = \"output_ids\" in batch[0]\n",
    "    input_ids_list = []\n",
    "    output_ids_list = []\n",
    "    IDs_list = []\n",
    "    \n",
    "    for item in batch:\n",
    "        input_ids_list.append(item[\"input_ids\"])\n",
    "        if has_output:\n",
    "            output_ids_list.append(item[\"output_ids\"])\n",
    "        else:\n",
    "            IDs_list.append(item[\"ID\"])\n",
    "    \n",
    "    input_ids_padded = nn.utils.rnn.pad_sequence(input_ids_list, batch_first=True, padding_value=0)\n",
    "    \n",
    "    if has_output:\n",
    "        output_ids_padded = nn.utils.rnn.pad_sequence(output_ids_list, batch_first=True, padding_value=0)\n",
    "        return input_ids_padded, output_ids_padded\n",
    "    else:\n",
    "        return input_ids_padded, IDs_list\n",
    "\n",
    "\n",
    "#######################################\n",
    "# 6) Seq2Seq (GRU) 모델\n",
    "#######################################\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, hidden = self.gru(embedded, hidden)\n",
    "        return outputs, hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward_step(self, input_token, hidden):\n",
    "        \"\"\"\n",
    "        input_token: (B, 1)\n",
    "        hidden: (1, B, hidden_size)\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_token)   # (B,1,E)\n",
    "        outputs, hidden = self.gru(embedded, hidden)  # (B,1,H)\n",
    "        logits = self.out(outputs)  # (B,1,vocab_size)\n",
    "        return logits, hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, pad_idx):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(vocab_size, embed_dim, hidden_size, pad_idx)\n",
    "        self.decoder = Decoder(vocab_size, embed_dim, hidden_size, pad_idx)\n",
    "    \n",
    "    def encode(self, src_ids):\n",
    "        return self.encoder(src_ids)\n",
    "\n",
    "    # 기존 self.forward(...)은 사용하지 않고,\n",
    "    # Teacher Forcing용 step-by-step 방식은 학습 루프에서 직접 구현.\n",
    "    # (원한다면 아래 forward를 그대로 두어도 되지만, 여기선 생략)\n",
    "    # def forward(self, src_ids, tgt_ids):\n",
    "    #     pass\n",
    "\n",
    "\n",
    "#######################################\n",
    "# 7) Training Loop for Seq2Seq (Teacher Forcing Ratio 적용)\n",
    "#######################################\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion, device, teacher_forcing_ratio=0.2):\n",
    "    \"\"\"\n",
    "    - Encoder -> hidden\n",
    "    - Decoder를 time-step별로 호출하면서,\n",
    "      teacher forcing 비율(teacher_forcing_ratio)로 정답 토큰 vs 이전 예측 토큰을 섞어서 입력\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids, output_ids = batch  # (B, T_in), (B, T_out)\n",
    "        input_ids = input_ids.to(device)\n",
    "        output_ids = output_ids.to(device)\n",
    "\n",
    "        # 1) 인코더\n",
    "        _, hidden = model.encode(input_ids)  # hidden: (1, B, H)\n",
    "        B, T_out = output_ids.shape\n",
    "\n",
    "        # 2) 디코더를 time-step별로 호출\n",
    "        # 첫 token은 output_ids[:, 0] (ex: [CLS])라 가정\n",
    "        dec_input = output_ids[:, 0].unsqueeze(1)  # (B,1)\n",
    "        \n",
    "        # 로짓을 저장할 tensor\n",
    "        # (B,T_out, vocab_size)\n",
    "        all_logits = torch.zeros(B, T_out, model.decoder.out.out_features, device=device)\n",
    "\n",
    "        for t in range(1, T_out):\n",
    "            logits_t, hidden = model.decoder.forward_step(dec_input, hidden)\n",
    "            # logits_t: (B,1,vocab_size)\n",
    "            all_logits[:, t, :] = logits_t[:, 0, :]  # (B,vocab_size)에 해당\n",
    "\n",
    "            # 다음 step의 dec_input 결정 (teacher forcing)\n",
    "            use_tf = (random.random() < teacher_forcing_ratio)\n",
    "            if use_tf:\n",
    "                # 정답 토큰 사용\n",
    "                dec_input = output_ids[:, t].unsqueeze(1)  # (B,1)\n",
    "            else:\n",
    "                # 이전 예측을 입력\n",
    "                dec_input = logits_t.argmax(dim=-1)  # (B,1)\n",
    "        \n",
    "        # 3) Loss 계산 (t=0 부분은 대부분 [CLS]이므로 t=1~T_out-1만 계산하거나, 그냥 전체 계산)\n",
    "        vocab_size = all_logits.shape[-1]\n",
    "        # 아래에서는 전체 cross-entropy 계산\n",
    "        loss = criterion(\n",
    "            all_logits.view(-1, vocab_size), \n",
    "            output_ids.view(-1)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def train_model(model, train_loader, config):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # pad=0\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        avg_loss = train_one_epoch(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            device=config.device,\n",
    "            teacher_forcing_ratio=Config.teacher_forcing_ratio\n",
    "        )\n",
    "        print(f\"[Epoch {epoch+1}/{config.num_epochs}] Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "#######################################\n",
    "# 8) Inference\n",
    "#######################################\n",
    "def greedy_decode(model, src_ids, tokenizer, max_len, pad_idx, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, hidden = model.encode(src_ids.to(device))  # (1,B,H)\n",
    "        \n",
    "        B = src_ids.size(0)\n",
    "        generated_ids = []\n",
    "        \n",
    "        # 첫 입력을 [CLS]라고 가정\n",
    "        dec_input = torch.tensor([tokenizer.cls_token_id]*B, dtype=torch.long, device=device).unsqueeze(1)\n",
    "        \n",
    "        for t in range(max_len - 1):\n",
    "            logits_t, hidden = model.decoder.forward_step(dec_input, hidden)  # (B,1,vocab)\n",
    "            next_token_id = logits_t.argmax(-1)  # (B,1)\n",
    "            generated_ids.append(next_token_id[:,0])  # list of (B,) tensors\n",
    "\n",
    "            dec_input = next_token_id  # next_input\n",
    "        \n",
    "        # (max_len-1, B) 형태 -> (B, max_len-1)\n",
    "        # 스택 후, transpose\n",
    "        all_gen = torch.stack(generated_ids, dim=0).transpose(0,1)  # (B, max_len-1)\n",
    "\n",
    "        # 여기서는 batch_size=1씩 돌린다고 했으므로, all_gen[0] 사용\n",
    "        # 만약 배치 처리를 하려면 for문으로 각각 디코딩\n",
    "        seq_out = all_gen[0].tolist()  # 첫 배치\n",
    "        text = tokenizer.decode(seq_out, skip_special_tokens=True)\n",
    "        return text\n",
    "\n",
    "def inference(model, test_loader, tokenizer, config):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for batch in test_loader:\n",
    "        input_ids, IDs = batch\n",
    "        input_ids = input_ids.to(config.device)\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            single_src = input_ids[i].unsqueeze(0)\n",
    "            ID = IDs[i]\n",
    "            pred_text = greedy_decode(model, single_src, tokenizer, seq_len, tokenizer.pad_token_id, config.device)\n",
    "            results.append({\"ID\": ID, \"output\": pred_text})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded fine-tuned Seq2Seq model.\n",
      "Submission file saved: ../database/submission/submission_seq2seq.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "#######################################\n",
    "# Config (Inference 설정)\n",
    "#######################################\n",
    "class InferenceConfig:\n",
    "    test_path = \"../database/test/test.csv\"\n",
    "    submission_path = \"../database/submission/sample_submission.csv\"\n",
    "    output_submission_path = \"../database/submission/submission_seq2seq.csv\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#######################################\n",
    "# 1) Fine-tuned 모델 불러오기\n",
    "#######################################\n",
    "# 기존 vocab 및 tokenizer 사용\n",
    "vocab = build_train_based_vocab(Config.train_path)\n",
    "tokenizer = SyllableTokenizer(\n",
    "    vocab,\n",
    "    pad_token=Config.PAD_TOKEN,\n",
    "    unk_token=Config.UNK_TOKEN,\n",
    "    cls_token=Config.CLS_TOKEN,\n",
    "    sep_token=Config.SEP_TOKEN\n",
    ")\n",
    "\n",
    "# Fine-tuned Seq2Seq 모델 로드\n",
    "model = Seq2Seq(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=Config.embedding_dim,\n",
    "    hidden_size=Config.hidden_size,\n",
    "    pad_idx=tokenizer.pad_token_id\n",
    ").to(InferenceConfig.device)\n",
    "\n",
    "# 저장된 fine-tuned 모델 가중치 불러오기\n",
    "model.load_state_dict(torch.load(\"seq2seq_finetuned.pt\"))\n",
    "model.eval()  # Inference 모드\n",
    "\n",
    "print(\"Loaded fine-tuned Seq2Seq model.\")\n",
    "\n",
    "#######################################\n",
    "# 2) 테스트 데이터 로드 및 Inference 수행\n",
    "#######################################\n",
    "df_test = pd.read_csv(InferenceConfig.test_path)\n",
    "test_dataset = ObfuscatedKoreanDataset(df_test, tokenizer, max_length=Config.max_length, is_train=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Inference 실행\n",
    "results = inference(model, test_loader, tokenizer, Config)\n",
    "\n",
    "#######################################\n",
    "# 3) Sample Submission 파일 업데이트\n",
    "#######################################\n",
    "df_submission = pd.read_csv(InferenceConfig.submission_path)\n",
    "\n",
    "# ✅ `output` 컬럼을 예측 결과로 업데이트\n",
    "id_to_pred = {res[\"ID\"]: res[\"output\"] for res in results}\n",
    "df_submission[\"output\"] = df_submission[\"ID\"].map(id_to_pred)\n",
    "\n",
    "# ✅ 최종 파일 저장\n",
    "df_submission.to_csv(InferenceConfig.output_submission_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"Submission file saved: {InferenceConfig.output_submission_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
